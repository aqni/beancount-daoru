name: "setup llama.cpp"
description: "setup llama.cpp and cache models"
inputs:
    version:
        description: "llama.cpp version, blank for latest"
        required: false
    hf-models:
        description: "huggingface model ids, separated by newline"
        required: false

runs:
    using: "composite"
    steps:
        - name: Setup llama.cpp to PATH
          uses: pbrisbin/setup-tool-action@v2
          with:
              name: llama
              version: ${{ inputs.version }}
              url: "https://github.com/ggml-org/llama.cpp/releases/download/{version}/{name}-{version}-bin-{os}-{arch}.{ext}"
              os-win32: win-cpu
              os-linux: ubuntu
              os-darwin: macos
              subdir-linux: "{name}-{version}"
              subdir-darwin: "{name}-{version}"

        # ref: https://github.com/ggml-org/llama.cpp/blob/4b2a4778f81f222c12271ce3b1997990b3071faf/common/common.cpp#L889
        - name: Determine cache directory based on OS
          id: determine-llama-cache
          shell: bash
          run: |
              if [ -n "$LLAMA_CACHE" ]
              then
                  cache_dir="$LLAMA_CACHE"
              else
                case "$RUNNER_OS" in
                    Linux)
                        if [ -n "$XDG_CACHE_HOME" ]; then
                            cache_dir="$XDG_CACHE_HOME/llama.cpp"
                        else
                            cache_dir="$HOME/.cache/llama.cpp"
                        fi
                        ;;
                    macOS)
                        cache_dir="$HOME/Library/Caches/llama.cpp"
                        ;;
                    Windows)
                        cache_dir="$LOCALAPPDATA\\llama.cpp"
                        ;;
                    *)
                        echo "Unsupported OS: $RUNNER_OS"
                        exit 1
                        ;;
                esac
              fi
              echo "llama-cache=$cache_dir" >> $GITHUB_OUTPUT
              echo "Detected llama.cpp cache path: $cache_dir"

        - name: Cache models
          uses: actions/cache@v5
          with:
              path: ${{ steps.determine-llama-cache.outputs.llama-cache }}
              key: llama.cpp-${{ inputs.hf-models }}
              enableCrossOsArchive: true

        - name: Download models
          shell: bash
          run: |
              while IFS= read -r hf_model; do
                  if [ -n "$hf_model" ]; then
                      echo "Caching hf-model $hf_model"
                      llama-cli -hf "$hf_model" -st -p "hello"
                  fi
              done <<< "${{ inputs.hf-models }}"
